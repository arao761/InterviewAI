"""
Test Setup Script
Verify Phase 1 setup is working correctly
"""

import os
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent))

from dotenv import load_dotenv

# Load environment variables
load_dotenv()


def test_environment():
    """Test that environment variables are set"""
    print("=" * 60)
    print("PHASE 1: Testing Environment Configuration")
    print("=" * 60)

    api_key = os.getenv("OPENAI_API_KEY")

    if not api_key:
        print("âŒ OPENAI_API_KEY not set!")
        print("   Please create .env file and add your API key")
        print("   Copy .env.example to .env and fill in your key")
        return False

    print(f"âœ… OPENAI_API_KEY is set (length: {len(api_key)})")

    model = os.getenv("DEFAULT_MODEL", "gpt-4")
    print(f"âœ… Default model: {model}")

    temperature = os.getenv("TEMPERATURE", "0.0")
    print(f"âœ… Temperature: {temperature}")

    return True


def test_imports():
    """Test that all required packages can be imported"""
    print("\n" + "=" * 60)
    print("Testing Package Imports")
    print("=" * 60)

    required_packages = [
        ("openai", "OpenAI"),
        ("anthropic", "Anthropic"),
        ("pydantic", "Pydantic"),
        ("dotenv", "python-dotenv"),
        ("tenacity", "Tenacity"),
    ]

    all_success = True

    for package_name, display_name in required_packages:
        try:
            __import__(package_name)
            print(f"âœ… {display_name} installed")
        except ImportError:
            print(f"âŒ {display_name} NOT installed")
            print(f"   Run: pip install {package_name}")
            all_success = False

    return all_success


def test_llm_client():
    """Test LLM client functionality"""
    print("\n" + "=" * 60)
    print("Testing LLM Client")
    print("=" * 60)

    try:
        from src.utils.llm_client import LLMClient

        print("âœ… LLMClient imported successfully")

        # Initialize client
        client = LLMClient(provider="openai", model="gpt-4o-mini")
        print(f"âœ… LLMClient initialized: {client}")

        # Test basic generation
        print("\nğŸ“¡ Testing API connection with simple prompt...")
        response = client.generate(
            prompt="Say 'Hello from PrepWise AI!' in exactly those words.",
            temperature=0.0,
            max_tokens=50
        )
        print(f"âœ… API Response: {response}")

        # Test JSON generation
        print("\nğŸ“¡ Testing JSON generation...")
        json_response = client.generate_json(
            prompt="Return a JSON object with two fields: 'status' (set to 'success') and 'message' (set to 'PrepWise AI is ready')"
        )
        print(f"âœ… JSON Response: {json_response}")

        assert json_response.get("status") == "success", "JSON parsing failed"
        print("âœ… JSON parsing working correctly")

        # Test token counting
        text = "This is a test of the token counting functionality."
        token_count = client.count_tokens(text)
        print(f"âœ… Token counting: '{text}' = ~{token_count} tokens")

        return True

    except ImportError as e:
        print(f"âŒ Failed to import LLMClient: {e}")
        return False
    except Exception as e:
        print(f"âŒ Error testing LLM client: {e}")
        print(f"   Error type: {type(e).__name__}")
        return False


def test_schemas():
    """Test Pydantic schemas"""
    print("\n" + "=" * 60)
    print("Testing Pydantic Schemas")
    print("=" * 60)

    try:
        from src.resume_parser.schemas import (
            Contact, Education, Experience, Skills, ParsedResume
        )

        print("âœ… All schemas imported successfully")

        # Test Contact
        contact = Contact(
            name="John Doe",
            email="john@example.com",
            phone="555-1234"
        )
        print(f"âœ… Contact schema: {contact.name}")

        # Test Skills
        skills = Skills(
            technical=["Python", "JavaScript"],
            tools=["Git", "Docker"]
        )
        print(f"âœ… Skills schema: {len(skills.technical)} technical skills")

        # Test ParsedResume
        resume = ParsedResume(
            contact=contact,
            skills=skills,
            total_years_experience=3.5
        )
        print(f"âœ… ParsedResume schema: {resume.experience_level} level")
        print(f"âœ… Resume summary: {resume.get_summary()}")

        return True

    except Exception as e:
        print(f"âŒ Error testing schemas: {e}")
        return False


def test_validators():
    """Test validator functions"""
    print("\n" + "=" * 60)
    print("Testing Validators")
    print("=" * 60)

    try:
        from src.utils.validators import (
            validate_email,
            validate_url,
            sanitize_text,
            validate_score
        )

        print("âœ… Validators imported successfully")

        # Test email validation
        assert validate_email("test@example.com") == True
        assert validate_email("invalid-email") == False
        print("âœ… Email validation working")

        # Test URL validation
        assert validate_url("https://example.com") == True
        assert validate_url("not-a-url") == False
        print("âœ… URL validation working")

        # Test text sanitization
        dirty_text = "  Too   much    space  "
        clean_text = sanitize_text(dirty_text)
        assert clean_text == "Too much space"
        print("âœ… Text sanitization working")

        # Test score validation
        assert validate_score(75) == True
        assert validate_score(150) == False
        print("âœ… Score validation working")

        return True

    except Exception as e:
        print(f"âŒ Error testing validators: {e}")
        return False


def test_question_generator():
    """Test Question Generator (Phase 3)"""
    print("\n" + "=" * 60)
    print("Testing Question Generator (Phase 3)")
    print("=" * 60)

    try:
        from src.question_generator.generator import QuestionGenerator
        from src.question_generator.schemas import (
            QuestionGenerationRequest, 
            QuestionType,
            DifficultyLevel,
            InterviewQuestion,
            QuestionSet
        )
        
        print("âœ… Question Generator modules imported successfully")
        
        # Test initialization
        generator = QuestionGenerator()
        print("âœ… QuestionGenerator initialized")
        
        # Test basic question generation
        request = QuestionGenerationRequest(
            target_role="Software Engineer",
            target_level="mid",
            num_technical=2,
            num_behavioral=1
        )
        
        result = generator.generate_questions(request)
        print(f"âœ… Generated {len(result.questions)} questions")
        
        # Verify question structure
        if result.questions:
            q = result.questions[0]
            assert hasattr(q, 'question'), "Question missing 'question' field"
            assert hasattr(q, 'type'), "Question missing 'type' field"
            assert hasattr(q, 'difficulty'), "Question missing 'difficulty' field"
            print("âœ… Question structure validated")
            
            # Test filtering
            tech_questions = result.get_questions_by_type(QuestionType.TECHNICAL)
            print(f"âœ… Question filtering works: {len(tech_questions)} technical questions")
        
        # Test session ID is unique
        result2 = generator.generate_questions(request)
        assert result.session_id != result2.session_id, "Session IDs should be unique"
        print("âœ… Unique session IDs generated")
        
        print("\nâœ… PASS - Question Generator")
        return True
        
    except ImportError as e:
        print(f"âŒ FAIL - Question Generator: Import error: {e}")
        return False
    except Exception as e:
        print(f"âŒ FAIL - Question Generator: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_answer_evaluator():
    """Test Answer Evaluator (Phase 4)"""
    print("\n" + "=" * 60)
    print("Testing Answer Evaluator (Phase 4)")
    print("=" * 60)

    try:
        from src.evaluator.evaluator import AnswerEvaluator
        from src.evaluator.schemas import (
            EvaluationRequest,
            AnswerEvaluation,
            ScoreLevel,
            EvaluationCriteria
        )
        
        print("âœ… Answer Evaluator modules imported successfully")
        
        # Test initialization
        evaluator = AnswerEvaluator()
        print("âœ… AnswerEvaluator initialized")
        
        # Test basic evaluation
        request = EvaluationRequest(
            question="What is a hash table?",
            answer="""A hash table is a data structure that provides O(1) average-case 
            lookup by using a hash function to map keys to array indices. It handles 
            collisions using chaining or open addressing.""",
            question_type="technical",
            expected_answer_points=[
                "Hash function",
                "O(1) lookup",
                "Collision handling"
            ]
        )
        
        evaluation = evaluator.evaluate_answer(request)
        print(f"âœ… Generated evaluation with score: {evaluation.overall_score}/100")
        
        # Verify evaluation structure
        assert hasattr(evaluation, 'overall_score'), "Missing overall_score"
        assert hasattr(evaluation, 'score_level'), "Missing score_level"
        assert hasattr(evaluation, 'strengths'), "Missing strengths"
        assert hasattr(evaluation, 'weaknesses'), "Missing weaknesses"
        print("âœ… Evaluation structure validated")
        
        # Test score level
        assert evaluation.score_level in [ScoreLevel.EXCELLENT, ScoreLevel.GOOD, ScoreLevel.FAIR, ScoreLevel.POOR]
        print(f"âœ… Score level: {evaluation.score_level.value}")
        
        # Test feedback generation
        total_feedback = len(evaluation.strengths) + len(evaluation.weaknesses) + len(evaluation.suggestions)
        assert total_feedback > 0, "No feedback generated"
        print(f"âœ… Generated {total_feedback} feedback items")
        
        # Test criterion scores
        assert len(evaluation.criterion_scores) > 0, "No criterion scores"
        print(f"âœ… Generated {len(evaluation.criterion_scores)} criterion scores")
        
        print("\nâœ… PASS - Answer Evaluator")
        return True
        
    except ImportError as e:
        print(f"âŒ FAIL - Answer Evaluator: Import error: {e}")
        return False
    except Exception as e:
        print(f"âŒ FAIL - Answer Evaluator: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_session_manager():
    """Test Session Manager & Progress Tracking (Phase 5 & 6)"""
    print("\n" + "=" * 60)
    print("Testing Session Manager & Progress Tracking (Phase 5 & 6)")
    print("=" * 60)

    try:
        from src.session_manager.manager import SessionManager
        from src.session_manager.schemas import (
            SessionCreateRequest,
            SessionStatus,
            InterviewMode
        )
        
        print("âœ… Session Manager modules imported successfully")
        
        # Test initialization
        manager = SessionManager()
        print("âœ… SessionManager initialized")
        
        # Test session creation
        request = SessionCreateRequest(
            candidate_name="Test User",
            user_id="test_user_123",
            target_role="Software Engineer",
            experience_level="mid",
            mode=InterviewMode.PRACTICE,
            num_technical=2,
            num_behavioral=1
        )
        
        session = manager.create_session(request)
        print(f"âœ… Created session: {session.session_id}")
        
        # Verify session structure
        assert session.session_id.startswith("sess_"), "Invalid session ID"
        assert session.candidate_name == "Test User", "Incorrect candidate name"
        assert session.total_questions == 3, "Incorrect question count"
        assert session.status == SessionStatus.SCHEDULED, "Incorrect initial status"
        print("âœ… Session structure validated")
        
        # Test starting session
        manager.start_session(session.session_id)
        updated_session = manager.get_session(session.session_id)
        assert updated_session.status == SessionStatus.IN_PROGRESS
        print("âœ… Session started successfully")
        
        # Test submitting answer
        response = manager.submit_answer(
            session.session_id,
            0,
            "A hash table uses a hash function to map keys to values efficiently",
            200
        )
        assert response.evaluation_score is not None
        print(f"âœ… Answer evaluated with score: {response.evaluation_score}/100")
        
        # Test user progress
        progress = manager.get_user_progress("test_user_123")
        assert progress.user_id == "test_user_123"
        assert progress.total_sessions >= 1
        print(f"âœ… User progress tracked: {progress.total_sessions} sessions")
        
        print("\nâœ… PASS - Session Manager & Progress Tracking")
        return True
        
    except ImportError as e:
        print(f"âŒ FAIL - Session Manager: Import error: {e}")
        return False
    except Exception as e:
        print(f"âŒ FAIL - Session Manager: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """Run all tests"""
    print("\n" + "=" * 60)
    print("PrepWise AI - Setup Verification (Phases 1-6)")
    print("=" * 60 + "\n")

    results = []

    # Run all tests
    results.append(("Environment", test_environment()))
    results.append(("Package Imports", test_imports()))
    results.append(("Pydantic Schemas", test_schemas()))
    results.append(("Validators", test_validators()))
    results.append(("LLM Client", test_llm_client()))
    results.append(("Question Generator", test_question_generator()))
    results.append(("Answer Evaluator", test_answer_evaluator()))
    results.append(("Session Manager & Progress", test_session_manager()))

    # Summary
    print("\n" + "=" * 60)
    print("TEST SUMMARY")
    print("=" * 60)

    for test_name, success in results:
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"{status} - {test_name}")

    total_passed = sum(1 for _, success in results if success)
    total_tests = len(results)

    print(f"\nTotal: {total_passed}/{total_tests} tests passed")

    if total_passed == total_tests:
        print("\nğŸ‰ All tests passed! Phases 1-6 complete!")
        print("\nCompleted Phases:")
        print("âœ… Phase 1: Core Infrastructure")
        print("âœ… Phase 2: Resume Parser")
        print("âœ… Phase 3: Question Generator")
        print("âœ… Phase 4: Answer Evaluator")
        print("âœ… Phase 5: Session Manager")
        print("âœ… Phase 6: Progress Tracking & Analytics")
        print("\nğŸš€ PrepWise AI is production-ready!")
        print("\nNext steps:")
        print("1. Build web API (FastAPI) or CLI interface")
        print("2. Add database integration (PostgreSQL/MongoDB)")
        print("3. Deploy to cloud (AWS/Azure/GCP)")
        print("4. Add user authentication and authorization")
        return 0
    else:
        print("\nâš ï¸  Some tests failed. Please fix the issues above.")
        return 1


if __name__ == "__main__":
    sys.exit(main())
